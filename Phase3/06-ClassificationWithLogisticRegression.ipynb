{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Recognize the model-less baseline understanding for a classification task\n",
    "- Use logistic regression to perform a classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For our modeling steps\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task: predict who will survive the sinking of the Titanic, given the passenger manifest. So we are predicting the column `survived`\n",
    "\n",
    "Dataset details: https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass                                             name     sex      age  \\\n",
       "0     1.0                    Allen, Miss. Elisabeth Walton  female  29.0000   \n",
       "1     1.0                   Allison, Master. Hudson Trevor    male   0.9167   \n",
       "2     1.0                     Allison, Miss. Helen Loraine  female   2.0000   \n",
       "3     1.0             Allison, Mr. Hudson Joshua Creighton    male  30.0000   \n",
       "4     1.0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female  25.0000   \n",
       "\n",
       "   sibsp  parch  ticket      fare    cabin embarked survived  \n",
       "0    0.0    0.0   24160  211.3375       B5        S        1  \n",
       "1    1.0    2.0  113781  151.5500  C22 C26        S        1  \n",
       "2    1.0    2.0  113781  151.5500  C22 C26        S        0  \n",
       "3    1.0    2.0  113781  151.5500  C22 C26        S        0  \n",
       "4    1.0    2.0  113781  151.5500  C22 C26        S        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the data from sklearn\n",
    "dfX, dfy = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "# Cleaning a bit to get to a full dataframe of the data\n",
    "df = dfX.copy()\n",
    "df = df.drop(columns=['boat', 'body', 'home.dest'])\n",
    "df['survived'] = dfy\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   pclass    1309 non-null   float64 \n",
      " 1   name      1309 non-null   object  \n",
      " 2   sex       1309 non-null   category\n",
      " 3   age       1046 non-null   float64 \n",
      " 4   sibsp     1309 non-null   float64 \n",
      " 5   parch     1309 non-null   float64 \n",
      " 6   ticket    1309 non-null   object  \n",
      " 7   fare      1308 non-null   float64 \n",
      " 8   cabin     295 non-null    object  \n",
      " 9   embarked  1307 non-null   category\n",
      " 10  survived  1309 non-null   category\n",
      "dtypes: category(3), float64(5), object(3)\n",
      "memory usage: 86.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Less Baseline\n",
    "\n",
    "As always, we want to know what we're up against: How hard is our problem? Do we even need to use a model here?\n",
    "\n",
    "Can we think of a strategy, without modeling, to guess whether a person survives the Titanic?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.618029\n",
       "1    0.381971\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement the strategy here!\n",
    "df[\"survived\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our predictions as something list-like\n",
    "model_less_preds = [\"0\"] * len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6180290297937356"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn's accuracy_score to compare actual to predictions\n",
    "accuracy_score(df[\"survived\"], model_less_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling with Logistic Regression!\n",
    "\n",
    "Let's build our first model! What features should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pclass         0\n",
       "name           0\n",
       "sex            0\n",
       "age          263\n",
       "sibsp          0\n",
       "parch          0\n",
       "ticket         0\n",
       "fare           1\n",
       "cabin       1014\n",
       "embarked       2\n",
       "survived       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the data to decide which columns to start with\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your initial X and y\n",
    "X = df[[\"pclass\", \"sex\", \"age\"]]\n",
    "y = df[\"survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Don't forget - SKLearn's implementation of Logistic Regression penalizes coefficients by default - at the very least, we'll need to scale our features!\n",
    "\n",
    "If we want to use any categorical or object-type columns as input variables, we'll need to encode them. We will also need to deal with any null values in columns we want to use.\n",
    "\n",
    "[A really useful resource/example of processing the Titanic data!](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Scale\n",
    "- Null data in `age`\n",
    "- Encode `sex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement preprocessing steps here\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"age\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "           (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"sex\", \"pclass\"]\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_pr = preprocessor.transform(X_train)\n",
    "X_test_pr = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Model!\n",
    "\n",
    "Now that we've preprocessed our data, let's model it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "# Fit our model\n",
    "logreg.fit(X_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7869520897043832"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate: use .score to check accuracy on train and test\n",
    "logreg.score(X_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7957317073170732"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_pr, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29218654,  1.21849964, -1.21848316,  1.01409269, -0.04801239,\n",
       "        -0.96606382]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out our coefficients - what do we learn?\n",
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating to Optimize Hyperparameters\n",
    "\n",
    "We've built an initial model, and now there are a lot of ways we can potentially improve our models. For example, we could use more features, drop less useful features, engineer smarter features, etc.\n",
    "\n",
    "BUT one thing we haven't explored so far is optimizing hyperparameters! Specifically, we might think about optimizing how strong our regularization parameter - should we penalize our model more? less? \n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C      | train score        | test score\n",
      "1000.0 | 0.7869520897043832 | 0.7957317073170732\n",
      "100.01 | 0.7869520897043832 | 0.7957317073170732\n",
      "10.001 | 0.7869520897043832 | 0.7957317073170732\n",
      "1.0001 | 0.7869520897043832 | 0.7957317073170732\n",
      "0.9001 | 0.7869520897043832 | 0.7957317073170732\n",
      "0.8001 | 0.7869520897043832 | 0.7957317073170732\n",
      "0.7001 | 0.7869520897043832 | 0.7957317073170732\n",
      "0.6001 | 0.7869520897043832 | 0.7957317073170732\n",
      "0.5001 | 0.7869520897043832 | 0.7957317073170732\n",
      "0.4001 | 0.7869520897043832 | 0.7957317073170732\n",
      "0.3001 | 0.7859327217125383 | 0.7957317073170732\n",
      "0.2001 | 0.7859327217125383 | 0.7926829268292683\n",
      "0.1001 | 0.7849133537206932 | 0.7926829268292683\n",
      "0.0501 | 0.7828746177370031 | 0.7926829268292683\n",
      "0.0101 | 0.7961264016309888 | 0.7408536585365854\n",
      "0.0075 | 0.7961264016309888 | 0.7469512195121951\n",
      "0.0052 | 0.7686034658511722 | 0.7195121951219512\n",
      "0.0046 | 0.7471967380224261 | 0.6859756097560976\n",
      "0.0041 | 0.7308868501529052 | 0.6585365853658537\n",
      "0.0039 | 0.7206931702344547 | 0.6402439024390244\n",
      "0.0035 | 0.6901121304791029 | 0.600609756097561\n",
      "0.0033 | 0.6636085626911316 | 0.5792682926829268\n",
      "0.0031 | 0.6371049949031601 | 0.5609756097560976\n",
      "0.0025 | 0.6371049949031601 | 0.5609756097560976\n",
      "0.0011 | 0.6371049949031601 | 0.5609756097560976\n",
      "0.0005 | 0.6371049949031601 | 0.5609756097560976\n",
      "0.0001 | 0.6371049949031601 | 0.5609756097560976\n"
     ]
    }
   ],
   "source": [
    "# Let's use cross_validate to try out different regularization strengths\n",
    "# I envision this as a for loop, looping over different options for C \n",
    "# Don't forget to capture the outputs of each loop somewhere!\n",
    "C = [1000.0 ,100.01, 10.001, 1.0001, .9001, .8001, .7001, .6001, .5001, .4001, .3001, .2001, .1001, .0501, .0101, .0075, .0052, .0046, .0041, .0039, .0035, .0033, .0031, .0025, .0011, .0005, .0001]\n",
    "train_score = []\n",
    "test_score = []\n",
    "for c in C:\n",
    "    logreg = LogisticRegression(C=c, random_state=1000)\n",
    "    logreg.fit(X_train_pr, y_train)\n",
    "    train_score.append(logreg.score(X_train_pr, y_train))\n",
    "    test_score.append(logreg.score(X_test_pr, y_test))\n",
    "print(\"C      | train score        | test score\")\n",
    "for y, x in enumerate(train_score):\n",
    "    print(C[y],\"|\", train_score[y], \"|\", test_score[y])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.637105\n",
       "1    0.362895\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.560976\n",
       "1    0.439024\n",
       "Name: survived, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore our results\n",
    "fig, ax = plt.subplots()\n",
    "plt.xscale('log')\n",
    "ax.plot(C, train_score, label='train')\n",
    "ax.plot(C, test_score, label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a new model with the best regularization strength, on the full train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then evaluate on train and test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss!\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "## Level Up: Notes on Cost Functions, and Solutions To the Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unlike the least-squares problem for linear regression, no one has yet found a closed-form solution to the optimization problem presented by logistic regression. But even if one exists, the computation would no doubt be so complex that we'd be better off using some sort of approximation method instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But there's still a problem.\n",
    "\n",
    "Recall the cost function for linear regression: <br/><br/>\n",
    "$SSE = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i(y_i - (\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in}))^2$.\n",
    "\n",
    "This function, $SSE(\\vec{\\beta})$, is convex.\n",
    "\n",
    "If we plug in our new logistic equation for $\\hat{y}$, we get: <br/><br/>\n",
    "$SSE_{log} = \\Sigma_i(y_i - \\hat{y}_i)^2 = \\Sigma_i\\left(y_i - \\left(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_nx_{in})}}\\right)\\right)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Bad News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*This* function, $SSE_{log}(\\vec{\\beta})$, is [**not** convex](https://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c).\n",
    "\n",
    "That means that, if we tried to use gradient descent or some other approximation method that looks for the minimum of this function, we could easily find a local rather than a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note that the scikit-learn class *expects the user to specify the solver* to be used in calculating the coefficients. The default solver, [lbfgs](https://en.wikipedia.org/wiki/Limited-memory_BFGS), works well for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Good News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use **log-loss** instead:\n",
    "\n",
    "$\\mathcal{L}(\\vec{y}, \\hat{\\vec{y}}) = -\\frac{1}{N}\\Sigma^N_{i=1}\\left(y_iln(\\hat{y}_i)+(1-y_i)ln(1-\\hat{y}_i)\\right)$,\n",
    "\n",
    "where $\\hat{y}_i$ is the probability that $(x_{i1}, ... , x_{in})$ belongs to **class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**More resources on the log-loss function**:\n",
    "\n",
    "https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11\n",
    "\n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### ðŸ§  Knowledge Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Is a bigger value (more positive) better or worse than a smaller value?\n",
    "\n",
    "- What would the log-loss for one data point when the target is $0$ but we predict $1$?\n",
    "\n",
    "- What would the log-loss for one data point when the target is $0$ but we predict $0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Level Up: More on Interpreting Logistic Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logreg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do we interpret the coefficients of a logistic regression? For a linear regression, the situaton was like this:\n",
    "\n",
    "- Linear Regression: We construct the best-fit line and get a set of coefficients. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change in $y$.\n",
    "\n",
    "- Logistic Regression: We find the coefficients of the best-fit line by some approximation method. Suppose $\\beta_1 = k$. In that case we would expect a 1-unit change in $x_1$ to produce a $k$-unit change (not in $y$ but) in $ln\\left(\\frac{y}{1-y}\\right)$.\n",
    "\n",
    "We have:\n",
    "\n",
    "$\\ln\\left(\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}\\right) = \\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k$.\n",
    "\n",
    "Exponentiating both sides:\n",
    "\n",
    "$\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)} = e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right) + k}$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^{\\ln\\left(\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}\\right)}\\cdot e^k$ <br/><br/> $\\frac{y(x_1+1, ... , x_n)}{1-y(x_1+1, ... , x_n)}= e^k\\cdot\\frac{y(x_1, ... , x_n)}{1-y(x_1, ... , x_n)}$\n",
    "\n",
    "That is, the odds ratio at $x_1+1$ has increased by a factor of $e^k$ relative to the odds ratio at $x_1$.\n",
    "\n",
    "For more on interpretation, see [this page](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/binary-logistic-regression/interpret-the-results/all-statistics-and-graphs/coefficients/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Level UP:  Other Link Functions, Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Logistic regression's link function is the logit function, but different sorts of models use different link functions.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function) has a nice table of generalized linear model types and their associated link functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
